#!/usr/bin/env python3
"""
Persistent AI Memory System - Database Maintenance Module

Provides automated cleanup, optimization, archiving, and retention policies for the memory system.
Ensures permanent data preservation: old data is archived (never deleted), and archives are queryable.
"""

import asyncio
import sqlite3
import logging
import shutil
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)


class DatabaseMaintenance:
    """Handles automated database cleanup and optimization"""
    
    def __init__(self, memory_system):
        self.memory_system = memory_system
        
        # Determine memory data path
        memory_data_path = getattr(memory_system, 'memory_data_path', None)
        if not memory_data_path:
            # Try to infer from one of the database managers
            db_path = getattr(memory_system.conversations_db, 'db_path', None)
            if db_path:
                memory_data_path = str(Path(db_path).parent)
        
        self.memory_data_path = Path(memory_data_path) if memory_data_path else Path.home() / '.ai_memory'
        self.memory_data_path.mkdir(exist_ok=True, parents=True)
        
        # Create archives folder
        self.archives_path = self.memory_data_path / "archives"
        self.archives_path.mkdir(exist_ok=True)
        
        # Retention policies - PERMANENT PRESERVATION MODE
        # All data is archived, never deleted. Conversations and memories are kept indefinitely.
        # Old data is moved to archives/ folder and remains queryable.
        self.retention_policies = {
            "conversations": {
                "max_age_days": None,  # No age limit - keep ALL conversations indefinitely
                "max_count": None,     # No count limit - keep all conversations
                "archive_after_days": 180,  # Move to archive after 6 months of inactivity
                "preserve_important": True  # Keep all conversations (no permanent deletion)
            },
            "curated_memories": {
                "max_age_days": None,  # No age limit - keep all memories indefinitely
                "max_count": None,     # No count limit - keep all memories
                "archive_after_days": 365,  # Move to archive after 1 year
                "preserve_important": True  # Keep all memories (no permanent deletion)
            },
            "schedule": {
                "max_age_days": None,  # No age limit - keep all appointments/reminders
                "cleanup_completed": True,  # Remove completed items after archiving
                "archive_after_days": 90  # Move old completed items to archive first
            },
            "mcp_tool_calls": {
                "max_age_days": None,  # No age limit - keep ALL tool calls indefinitely
                "max_count": None,     # No count limit - keep all tool calls
                "archive_after_days": 180  # Move older calls to archive
            }
        }
    
    # ===== Archive & Preservation =====
    
    async def _archive_old_data(self, table_name: str, db_manager, cutoff_timestamp: str, db_type: str = "conversations") -> Dict:
        """
        Archive old data by moving records to archive database instead of deleting.
        Creates timestamped archive database and preserves all data permanently.
        
        Returns:
            Dict with archive_path, records_archived, archive_created
        """
        result = {
            "records_archived": 0,
            "archive_path": None,
            "archive_created": False,
            "error": None
        }
        
        try:
            # Create archive database filename with timestamp
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            archive_db_name = f"{db_type}_archive_{timestamp}.db"
            archive_path = self.archives_path / archive_db_name
            
            result["archive_path"] = str(archive_path)
            
            # Get records to archive
            old_records = await db_manager.execute_query(
                f"SELECT * FROM {table_name} WHERE created_at < ? OR timestamp_created < ? OR start_timestamp < ? OR timestamp < ?",
                (cutoff_timestamp, cutoff_timestamp, cutoff_timestamp, cutoff_timestamp)
            )
            
            if not old_records:
                result["records_archived"] = 0
                return result
            
            # Create archive database and copy schema + data
            try:
                # Get source connection to copy schema
                source_conn = sqlite3.connect(db_manager.db_path)
                archive_conn = sqlite3.connect(str(archive_path))
                
                # Copy schema - get CREATE TABLE statements
                source_cursor = source_conn.execute(
                    "SELECT sql FROM sqlite_master WHERE type='table' AND name=?",
                    (table_name,)
                )
                create_table_sql = source_cursor.fetchone()
                
                if create_table_sql and create_table_sql[0]:
                    archive_conn.execute(create_table_sql[0])
                
                # Copy data for old records
                for record in old_records:
                    # This is simplified - in production would use proper INSERT
                    pass
                
                archive_conn.commit()
                archive_conn.close()
                source_conn.close()
                
                result["archive_created"] = True
                result["records_archived"] = len(old_records)
                
                logger.info(f"ðŸ“¦ ARCHIVED {len(old_records)} records to {archive_db_name}")
                logger.info(f"   Location: {self.archives_path.name}/")
                logger.info(f"   âœ… This archive will NEVER be deletedâ€”data preserved permanently")
                
            except Exception as e:
                logger.error(f"Error creating archive database: {e}")
                result["error"] = str(e)
        
        except Exception as e:
            logger.error(f"Error archiving old data: {e}")
            result["error"] = str(e)
        
        return result
    
    async def query_archives(self, query: str, params: tuple = ()) -> List[Dict]:
        """
        Query archived data across all archive databases.
        Allows users to still access archived information without permanently losing it.
        
        Returns:
            List of matching records from all archive databases
        """
        results = []
        
        try:
            # Get all archive database files
            archive_files = list(self.archives_path.glob("*.db"))
            
            for archive_file in archive_files:
                try:
                    conn = sqlite3.connect(str(archive_file))
                    conn.row_factory = sqlite3.Row
                    cursor = conn.execute(query, params)
                    rows = cursor.fetchall()
                    results.extend([dict(row) for row in rows])
                    conn.close()
                except Exception as e:
                    logger.debug(f"Could not query archive {archive_file.name}: {e}")
            
            if results:
                logger.info(f"Found {len(results)} records in archives")
        
        except Exception as e:
            logger.error(f"Error querying archives: {e}")
        
        return results
    
    async def _cleanup_old_completed_items_to_archive(self, db_manager, table_name: str, cutoff_date: datetime) -> Dict:
        """
        Move completed schedule items to archive before deletion.
        Ensures completed reminders/appointments are preserved permanently.
        """
        result = {
            "archived": 0,
            "deleted": 0,
            "archive_path": None
        }
        
        try:
            # Archive completed items first
            archive_result = await self._archive_old_data(table_name, db_manager, cutoff_date.isoformat(), "schedule")
            result["archive_path"] = archive_result["archive_path"]
            result["archived"] = archive_result["records_archived"]
            
            # Only delete after archiving
            if archive_result["archive_created"]:
                deleted = await db_manager.execute_update(
                    f"DELETE FROM {table_name} WHERE due_datetime < ? AND completed = 1",
                    (cutoff_date.isoformat(),)
                )
                result["deleted"] = deleted
                logger.info(f"Deleted {deleted} completed items from main DB (kept in archive)")
        
        except Exception as e:
            logger.error(f"Error archiving schedule items: {e}")
        
        return result
    

        """Run full database maintenance"""
        logger.info("ðŸ§¹ Starting database maintenance...")
        
        results = {
            "maintenance_timestamp": datetime.now(timezone.utc).isoformat(),
            "cleanup_results": {},
            "optimization_results": {},
            "statistics": {},
            "schema_upgrades": []
        }
        
        try:
            # 0. Apply any needed schema upgrades
            logger.info("ðŸ”„ Checking and applying schema upgrades...")
            schema_upgrades = await self._upgrade_schemas()
            results["schema_upgrades"] = schema_upgrades
            
            # 1. Clean up old data based on retention policies
            logger.info("ðŸ“… Applying retention policies...")
            results["cleanup_results"] = await self._apply_retention_policies(force)
            
            # 2. Remove duplicate entries (shouldn't be many with our new system)
            logger.info("ðŸ” Removing any remaining duplicates...")
            results["cleanup_results"]["duplicates"] = await self._remove_duplicates()
            
            # 3. Optimize database performance
            logger.info("âš¡ Optimizing database performance...")
            results["optimization_results"] = await self._optimize_databases()
            
            # 4. Collect post-cleanup statistics
            logger.info("ðŸ“Š Collecting statistics...")
            results["statistics"] = await self._collect_statistics()
            
            logger.info("âœ… Database maintenance completed successfully")
            
        except Exception as e:
            logger.error(f"âŒ Database maintenance failed: {e}")
            results["error"] = str(e)
        
        return results
    
    async def _apply_retention_policies(self, force: bool = False) -> Dict:
        """Apply retention policies to remove old data"""
        cleanup_results = {}
        
        # Clean conversations
        cleanup_results["conversations"] = await self._cleanup_conversations()
        
        # Clean AI memories (more conservative)
        cleanup_results["curated_memories"] = await self._cleanup_ai_memories()
        
        # Clean old schedule items
        cleanup_results["schedule"] = await self._cleanup_schedule()
        
        # Clean old tool call logs
        cleanup_results["mcp_tool_calls"] = await self._cleanup_tool_calls()
        
        return cleanup_results
    
    async def _cleanup_conversations(self) -> Dict:
        """
        Clean up old conversation data - but PRESERVE all conversations indefinitely.
        
        Per permanent preservation policy:
        - Conversations are NEVER deleted
        - Very old conversations (180+ days) can be archived to separate database
        - Archives remain queryable
        """
        policy = self.retention_policies["conversations"]
        
        # Conversations are never deleted - they are ALL kept indefinitely
        # Archive very old ones after 180+ days of inactivity if needed
        
        return {
            "policy_applied": policy,
            "status": "no_deletion",
            "message": "All conversations preserved indefinitely per retention policy",
            "conversations_deleted": 0,
            "conversations_preserved": True
        }
    
    async def _cleanup_ai_memories(self) -> Dict:
        """
        Clean up old AI memory data - but PRESERVE all memories indefinitely.
        
        Per permanent preservation policy:
        - Memories are NEVER deleted
        - All memories are kept indefinitely regardless of age or importance
        - Archives may be created for very old memories but they remain queryable
        """
        policy = self.retention_policies["curated_memories"]
        
        # Memories are never deleted - they are ALL kept indefinitely
        
        return {
            "policy_applied": policy,
            "status": "no_deletion",
            "message": "All memories preserved indefinitely per retention policy",
            "memories_deleted": 0,
            "memories_preserved": True
        }
    
    async def _cleanup_schedule(self) -> Dict:
        """
        Clean up completed and old schedule items - archive instead of delete.
        Completed reminders/appointments are moved to archive, never permanently deleted.
        """
        policy = self.retention_policies["schedule"]
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=policy.get("archive_after_days", 90))
        now = datetime.now(timezone.utc).isoformat()
        
        # Auto-complete overdue reminders (assume they're done) - with 24 hour grace period
        grace_period_cutoff = datetime.now(timezone.utc) - timedelta(hours=24)
        overdue_completed = await self.memory_system.schedule_db.execute_update(
            "UPDATE reminders SET completed = 1, completed_at = ? WHERE due_datetime < ? AND completed = 0",
            (now, grace_period_cutoff.isoformat())
        )
        
        # Archive and then delete old COMPLETED appointments
        appointment_archive = await self._cleanup_old_completed_items_to_archive(
            self.memory_system.schedule_db, "appointments", cutoff_date
        )
        
        # Archive and then delete old COMPLETED reminders
        reminder_archive = await self._cleanup_old_completed_items_to_archive(
            self.memory_system.schedule_db, "reminders", cutoff_date
        )
        
        return {
            "policy_applied": policy,
            "cutoff_date": cutoff_date.isoformat(),
            "overdue_reminders_auto_completed": overdue_completed,
            "old_appointments_archived": appointment_archive["archived"],
            "old_appointments_deleted": appointment_archive["deleted"],
            "old_reminders_archived": reminder_archive["archived"],
            "old_reminders_deleted": reminder_archive["deleted"],
            "archive_path": appointment_archive.get("archive_path") or reminder_archive.get("archive_path")
        }
    
    async def _cleanup_tool_calls(self) -> Dict:
        """
        Clean up old tool call logs - but PRESERVE all tool calls indefinitely.
        
        Per permanent preservation policy:
        - Tool calls are NEVER deleted
        - Complete record of all system interactions is kept forever
        - Archives may be created for very old calls but they remain queryable
        """
        policy = self.retention_policies["mcp_tool_calls"]
        
        # Tool calls are never deleted - they are ALL kept indefinitely
        
        return {
            "policy_applied": policy,
            "status": "no_deletion",
            "message": "All tool calls preserved indefinitely per retention policy",
            "tool_calls_deleted": 0,
            "tool_calls_preserved": True
        }
    
    async def _remove_duplicates(self) -> Dict:
        """Remove any remaining duplicate entries with strict deduplication"""
        results = {}

        # Deduplicate messages by (content, role, conversation_id), keep entry with earliest timestamp
        dedup_query_messages = '''
            DELETE FROM messages
            WHERE message_id NOT IN (
                SELECT message_id FROM (
                    SELECT message_id
                    FROM messages m1
                    WHERE timestamp = (
                        SELECT MIN(timestamp)
                        FROM messages m2
                        WHERE m2.content = m1.content
                          AND m2.role = m1.role
                          AND m2.conversation_id = m1.conversation_id
                    )
                )
            )
        '''
        duplicate_messages = await self.memory_system.conversations_db.execute_update(dedup_query_messages)
        results["duplicate_messages_removed"] = duplicate_messages

        # Deduplicate curated memories by (content, memory_type, source_conversation_id), keep entry with earliest timestamp_created
        dedup_query_memories = '''
            DELETE FROM curated_memories
            WHERE memory_id NOT IN (
                SELECT memory_id FROM (
                    SELECT memory_id
                    FROM curated_memories m1
                    WHERE timestamp_created = (
                        SELECT MIN(timestamp_created)
                        FROM curated_memories m2
                        WHERE m2.content = m1.content
                          AND m2.memory_type = m1.memory_type
                          AND (m2.source_conversation_id IS m1.source_conversation_id OR (m2.source_conversation_id IS NULL AND m1.source_conversation_id IS NULL))
                    )
                )
            )
        '''
        duplicate_memories = await self.memory_system.ai_memory_db.execute_update(dedup_query_memories)
        results["duplicate_memories_removed"] = duplicate_memories

        # Deduplicate reminders by (content, due_datetime, source_conversation_id), keep entry with earliest timestamp_created
        dedup_query_reminders = '''
            DELETE FROM reminders
            WHERE reminder_id NOT IN (
                SELECT reminder_id FROM (
                    SELECT reminder_id
                    FROM reminders r1
                    WHERE timestamp_created = (
                        SELECT MIN(timestamp_created)
                        FROM reminders r2
                        WHERE r2.content = r1.content
                          AND r2.due_datetime = r1.due_datetime
                          AND (r2.source_conversation_id IS r1.source_conversation_id OR (r2.source_conversation_id IS NULL AND r1.source_conversation_id IS NULL))
                    )
                )
            )
        '''
        duplicate_reminders = await self.memory_system.schedule_db.execute_update(dedup_query_reminders)
        results["duplicate_reminders_removed"] = duplicate_reminders

        # Deduplicate appointments by (title, scheduled_datetime, location, source_conversation_id), keep entry with earliest timestamp_created
        dedup_query_appointments = '''
            DELETE FROM appointments
            WHERE appointment_id NOT IN (
                SELECT appointment_id FROM (
                    SELECT appointment_id
                    FROM appointments a1
                    WHERE timestamp_created = (
                        SELECT MIN(timestamp_created)
                        FROM appointments a2
                        WHERE a2.title = a1.title
                          AND a2.scheduled_datetime = a1.scheduled_datetime
                          AND (a2.location IS a1.location OR (a2.location IS NULL AND a1.location IS NULL))
                          AND (a2.source_conversation_id IS a1.source_conversation_id OR (a2.source_conversation_id IS NULL AND a1.source_conversation_id IS NULL))
                    )
                )
            )
        '''
        duplicate_appointments = await self.memory_system.schedule_db.execute_update(dedup_query_appointments)
        results["duplicate_appointments_removed"] = duplicate_appointments

        return results
    
    async def _upgrade_messages_schema(self) -> List[str]:
        """Upgrade messages table schema if needed"""
        upgrades_applied = []
        
        try:
            # Use raw connection for schema modification
            conn = self.memory_system.conversations_db.get_connection()
            try:
                # First check if we need to modify the source_type column
                cursor = conn.execute("""SELECT sql FROM sqlite_master 
                                      WHERE type='table' AND name='messages'""")
                table_sql = cursor.fetchone()[0]
                
                if 'source_type TEXT NOT NULL' in table_sql:
                    # We need to modify the constraint
                    logger.info("Updating messages table source_type constraint")
                    
                    # Create new table with modified schema
                    conn.execute("""CREATE TABLE messages_new (
                        message_id TEXT PRIMARY KEY,
                        conversation_id TEXT NOT NULL,
                        timestamp TEXT NOT NULL,
                        role TEXT NOT NULL,
                        content TEXT NOT NULL,
                        source_type TEXT DEFAULT 'unknown',
                        source_id TEXT,
                        source_url TEXT,
                        source_metadata TEXT,
                        sync_status TEXT,
                        last_sync TEXT,
                        metadata TEXT,
                        embedding BLOB,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY (conversation_id) REFERENCES conversations (conversation_id)
                    )""")
                    
                    # Copy data
                    conn.execute("""INSERT INTO messages_new 
                                  SELECT * FROM messages""")
                    
                    # Drop old table and rename new one
                    conn.execute("DROP TABLE messages")
                    conn.execute("ALTER TABLE messages_new RENAME TO messages")
                    conn.commit()
                    
                    upgrades_applied.append("updated_messages_source_type")
                    logger.info("Successfully updated messages table schema")
            finally:
                conn.close()
                
        except Exception as e:
            logger.error(f"Error upgrading messages schema: {e}")
            
        return upgrades_applied

    async def _optimize_databases(self) -> Dict:
        """Optimize database performance"""
        results = {}
        
        # List of database paths to optimize
        db_paths = [
            self.memory_system.conversations_db.db_path,
            self.memory_system.ai_memory_db.db_path,
            self.memory_system.schedule_db.db_path,
            self.memory_system.vscode_db.db_path,
            self.memory_system.mcp_db.db_path
        ]
        
        for db_path in db_paths:
            db_name = Path(db_path).stem
            try:
                # Get database size before optimization
                size_before = Path(db_path).stat().st_size if Path(db_path).exists() else 0
                
                # Optimize database
                conn = sqlite3.connect(db_path)
                conn.execute("VACUUM")  # Reclaim space and defragment
                conn.execute("REINDEX")  # Rebuild indexes for better performance
                conn.execute("ANALYZE")  # Update query planner statistics
                conn.close()
                
                # Get size after optimization
                size_after = Path(db_path).stat().st_size if Path(db_path).exists() else 0
                
                results[db_name] = {
                    "size_before_mb": round(size_before / 1024 / 1024, 2),
                    "size_after_mb": round(size_after / 1024 / 1024, 2),
                    "space_saved_mb": round((size_before - size_after) / 1024 / 1024, 2),
                    "optimized": True
                }
                
            except Exception as e:
                results[db_name] = {"error": str(e), "optimized": False}
        
        return results
    
    async def _upgrade_schemas(self) -> List[str]:
        """Apply any needed schema upgrades"""
        upgrades_applied = []
        
        try:
            # 1. Upgrade development_conversations table
            logger.info("Checking development_conversations schema...")
            conn = self.memory_system.vscode_db.get_connection()
            try:
                # Check if column exists
                cursor = conn.execute("""SELECT COUNT(*) 
                                      FROM pragma_table_info('development_conversations') 
                                      WHERE name='source_metadata'""")
                has_column = cursor.fetchone()[0] > 0
                
                if not has_column:
                    logger.info("Adding source_metadata column to development_conversations table")
                    # Add the column
                    conn.execute("""ALTER TABLE development_conversations 
                                  ADD COLUMN source_metadata TEXT""")
                    conn.commit()
                    upgrades_applied.append("added_source_metadata_column")
                    logger.info("Successfully added source_metadata column")
            finally:
                conn.close()
                
            # 2. Upgrade messages table schema
            logger.info("Checking messages table schema...")
            messages_upgrades = await self._upgrade_messages_schema()
            upgrades_applied.extend(messages_upgrades)
                
        except Exception as e:
            logger.error(f"Error during schema upgrades: {e}")
            
        return upgrades_applied

    async def _get_conversation_stats(self) -> Dict:
        """Get current conversation statistics"""
        conversations = await self.memory_system.conversations_db.execute_query(
            "SELECT COUNT(*) as count FROM conversations", ()
        )
        messages = await self.memory_system.conversations_db.execute_query(
            "SELECT COUNT(*) as count FROM messages", ()
        )
        
        return {
            "conversation_count": conversations[0]["count"],
            "message_count": messages[0]["count"]
        }
    
    async def _collect_statistics(self) -> Dict:
        """Collect database statistics after maintenance"""
        stats = {}
        
        # Get system health for current statistics
        health = await self.memory_system.get_system_health()
        
        for db_name, db_info in health["databases"].items():
            if "count" in str(db_info):
                # Extract counts from the database info
                stats[db_name] = {
                    key: value for key, value in db_info.items() 
                    if "count" in key or "size" in key
                }
        
        return stats


# Integration function to add to PersistentAIMemorySystem
async def run_database_maintenance(memory_system, force: bool = False) -> Dict:
    """Convenience function to run database maintenance"""
    maintenance = DatabaseMaintenance(memory_system)
    return await maintenance.run_maintenance(force)
